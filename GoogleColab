# -*- coding: utf-8 -*-
"""MyCapstoneProject_U3259336.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gJK-gkqHeT5hW6A82IAxMkHgvGRXiIqy

# My Capstone Project - Assignment 9
*   Flight Price Prediction using Kaggle Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Mapping the Drive"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/MyCapstoneProject

!ls /content/drive/MyDrive/MyCapstoneProject/Clean_Dataset.csv

"""# Step 1: Reading The Dataset"""

# Suppressing the Warning Messages
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

#Read the dataset
df = pd.read_csv('/content/drive/MyDrive/MyCapstoneProject/Clean_Dataset.csv', encoding="latin")
print("The shape of the data before removing duplicate values: ", df.shape)

#Removing duplicate rows
df = df.drop_duplicates()
print("the shape of the data after removing duplicate values: ", df.shape)

#Print sample data
df.head(5)

"""Key Observations about the Data Description


*   The dataset contains 300,153 flight ticket details.
*   There are 11 attributes as outlined below:
              airline: The name of the airline company
              flight: The flight code
              source_city: city of departure (6 destinations)
              departure_time: Flight departure time broken up into (6) time period bins
              stops: The number of stops made (3 categories)
              arrival_time: Flight arrival time broken up into (6) time period bins
              destination_city: city of arrival (6 destination)
              class: Seat class (2 distinct values: Economy and Business)
              duration: Overall length of the flight (hrs)
              days_left: Days between the flight booking and trip date
              price: Price of fight ticket

# Step 2: Problem Statement Definition

*   Create a prediction model to predict the price of a flight ticket
*   Target Variable: Price
*   Predictors/Features: airline, departure_time, class, stops etc.

# Step 3 - Choosing the appropriate ML/AI Algorithm for Data Analysis

The target variable is continuous, therefore a supervised ML regression model will be created.

# Step 4 - Distribution of the Data
"""

#Display a histogram to assess distribution of the target variable
df["price"].hist(grid = False)

#Perform transformation analyais to determine the best approach to even the distribution.
def plot_transformations(df, col_name):
    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 8))
    fig.suptitle(f"Transformations for {col_name}")

    transformations = [
        ("Original", df[col_name]),
        ("Logarithm", np.log(df[col_name])),
        ("Square Root", np.sqrt(df[col_name])),
        ("Reciprocal", np.reciprocal(df[col_name])),
    ]

    for (title, transformed_data), ax in zip(transformations, axes.flat):
        ax.hist(transformed_data, label=f"Skewness: {round(transformed_data.skew(), 2)}")
        ax.set_title(title)
        ax.legend(loc="upper right")

    plt.tight_layout()

plot_transformations(df, "price")

df["price"] = np.log(df["price"])

"""Observations from Step 4


*   The distribution of the target variable had a positive skew of 1.69, therefore data transformation analysis has been undertaken to deteremine the approiate method to evenly distribute the data, concluding logarithms with a skew of 0.4 is the best approach for reduced skewness.
*   The number of rows are adequate for the model to learn from.

# Step 5: Basic Exploratory Data Analysis
"""

#Looking at the first 5 rows of data
df.head()

#Looking at the last 5 rows of data
df.tail()

#Observe the summarised information of data
df.info()

#Look at the descriptive statistics of the data
df.describe(include="all")

#Finding the unique values for each columns
df.nunique()

"""Observations from Step 5 - Basic Exploratory Data Analysis



*   All rows and columns contain data.
*   The columns examined in Step 5 (and listed below) are not final, and further study will be undertaken to determine final listing.
              airline: Categorical Selected.
              flight: Categorical Selected.
              source_city: Categorical Selected.
              departure_time: Categorical Selected.
              stops: Categorical Selected.
              arrival_time: Categorical Selected.
              destination_city: Categorical Selected.
              class: Categorical Selected.
              duration: Continuous Selected.
              days_left: Continuous Selected.
              price: Continuous Selected.

# Step 7: Remove Unwanted Columns and Convert Values
"""

#Remove unwanted catergorical columns
columns_delete = ["Unnamed: 0", "airline", "flight", "source_city", "arrival_time", "destination_city"]
for col in columns_delete:
    try:
      df = df.drop(col, axis=1)
    except KeyError:
      print(f"Column {col}, not found.")
df.head()

df["departure_time"].value_counts()

df["class"].value_counts()

df["stops"].value_counts()

#Converting class and stops columns to numerical type
mapping = {"departure_time": {"Morning": 0, "Early_Morning": 1, "Evening": 2, "Night": 3, "Afternoon": 4, "Late_Night": 5}, "class":
                      {"Economy": 0, "Business": 1}, "stops": {"zero": 0, "one": 1, "two_or_more": 2}}

for col, values in mapping.items():
    try:
        if col in df.columns:
            df[col].replace(values, inplace=True)
        else:
            print(f"Column {col} not found.")
    except:
        print(f"An error occurred while processing column {col}.")

df.head()

"""# Step 8: Visual Exporatory Data Analysis"""

def plotBarCharts(impData, colsToPlot):
    fig, subPlot = plt.subplots(nrows=1, ncols = len(colsToPlot), figsize=(15,5))
    fig.suptitle("Bar Chart of: "+ str(colsToPlot))

    for colName, ax in zip(colsToPlot, range(len(colsToPlot))):
      df.groupby(colName).size().plot(kind="bar", ax = subPlot[ax])

plotBarCharts(impData = df, colsToPlot = ["departure_time","class", "stops"])



"""Observations from Step 8 - Visual Exploratoy Data Analysis


*   There's enough values for observation.
*   1 stop has significantly more values than the other categories.

# Step 9: Visualise Distribution of all the Continuous Variables
"""

df.hist(["duration", "days_left"], figsize = (15,5), grid=False)

"""Observations for Step 9

duration:
*   Selected. Outliers beyond 35 will be treated.
*   The data contained within 'duration' is negatively skewed (left skewed).
---
days_left:
*   Selected. The distribution is good.

# Step 10: Outlier Analysis
"""

df["duration"][df["duration"]<35].sort_values(ascending = False)

df["duration"][df["duration"]>35] = 34.92

df.hist(["duration"], figsize = (10,6), grid = False)

"""Observations from Step 11


*   Analysis undertaken shows the nearest logical value to 35 is 34.92, therefore values above 35 have been replace with it.
*   The distribution has improve after outlier treatment.

# Step 12: Missing Values Analysis
"""

df.isnull().sum()

"""Observations for Step 12


*   The dataset contains no missing values within it's rows, therefore removal of data is not required.

# Step 13: Feature Selection (Attribute Selection)
"""

ContinuousCols = ["duration", "days_left"]
for predictor in ContinuousCols:
  df.plot.scatter(x = predictor, y = "price", figsize = (8,5), title = predictor + " VS " + "price")

"""Observations for Step 13

* There is not an obvious decernible pattern in the scatter plots.

# Step 14: Statistical Feature Selection (Continuous Vs Continous) using Correlation value
"""

continuousCols = ["price", "duration", "days_left"]

correlationData = df[continuousCols].corr()
correlationData

correlationData["price"][abs(correlationData["price"]) > 0.2 ]

"""Observations from Step 14

*   'days_left' with a correlation of -0.193535 is not correlated with the target variable, therefore will be dropped.
*   'duration' with a correlation of 0.26472 is correlated with the target variable therefore is selected.

# Step 15: Relationship Exploration: Categorical Vs Continuous -- Box Plot
"""

categoricalCols = ["departure_time","class", "stops"]

fig, plotCanvas = plt.subplots(nrows = 1, ncols = len(categoricalCols), figsize = (15,7))
for predictorCol, i in zip(categoricalCols, range(len(categoricalCols))):
  df.boxplot(column = "price", by = predictorCol, figsize = (5,5), vert = True, ax = plotCanvas[i])

"""Observations and Interpretations from Step 15: Box-Plots

departure_time:
*   The majority of boxes (Q3 - Q1) are large indicating the middle 50% is broadly spread.
*   'Late Night: 5', contains outliers.
*   The whiskers are mostly equal in length suggesting overall the data is evenly skewed.
---
class:
*   The boxes are small suggesting the middle 50% is compact and the distrbution is concenstrated.
*   Both variables contain a large number of outliers.
*   The whiskers are mostly equal in length suggesting overall the data is evenly skewed.
---
stops:
*   The box for 'stops 2' is larger than the other boxes. This is likely due to the variable containing more data points.
*   The larger number of outliers are found in variable 'stops 2'.
*   'stops 1' and 'stops 2' have whiskers that are equal in length. 'stops 1' has a larger upper whisker, suggesting a skewd distribution.

# Step 16: Statistical Feature Selction (Categorical Vs Continous) using ANOVA Test
"""

def FunctionAnova(inpData, TargetVariable, categoricalPredictor):
    from scipy.stats import f_oneway
    # Creating an empty list of final selected predictors
    SelectedPredictors=[]

    print('##### ANOVA Results ##### \n')
    for predictor in categoricalPredictor:
        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)
        AnovaResults = f_oneway(*CategoryGroupLists)

        # If the ANOVA P-Value is <0.05, that means we reject H0
        if (AnovaResults[1] < 0.05):
            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])
            SelectedPredictors.append(predictor)
        else:
            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])

    return(SelectedPredictors)

 #Calling the function to check which categorical variables are correlated with target
categoricalPredictor = ["departure_time","class", "stops"]
df[categoricalPredictor] = df[categoricalPredictor].astype("category")
FunctionAnova(inpData = df,
              TargetVariable = "price",
              categoricalPredictor = categoricalPredictor)

"""Observations from Step 16


*   All three variables (departure_time, class and stops) are correlated with the target variable, price.

# Selecting Final Predictors/Features for Building Machine Learning/AI Model

The final features/predictors/columns selected for the machine learning model are:


*   "duration", "departure_time", "class", "stops"
"""

SelectedColumns = ["duration", "departure_time", "class", "stops"]

DataForML = df[SelectedColumns]
DataForML.head()

DataForML.to_pickle("DataForML.pkl")

"""# Step 17: Data Pre-Processing for Machine Learning Model Building or Model Development"""

DataForML_Numeric = pd.get_dummies(DataForML)

DataForML_Numeric["price"] = df["price"]

DataForML_Numeric.head()

"""# Step 18: Machine Learning Model Development"""

DataForML_Numeric.columns

TargetVariable = "price"
Predictors = ['duration', 'departure_time_0', 'departure_time_1', 'departure_time_2', 'departure_time_3', 'departure_time_4', 'departure_time_5', 'class_0', 'class_1', 'stops_0', 'stops_1', 'stops_2']

x = DataForML_Numeric[Predictors].values
y = DataForML_Numeric[TargetVariable].values

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 428)

"""# Step 19: Standardisation/Normalisation of Data

I have decided not to run this step.

# Step 20: Multiple Linear Regression Algorithm For ML/AL Model Building
"""

from sklearn.linear_model import LinearRegression
from sklearn import metrics
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score

RegModel = LinearRegression()

print(RegModel)

LREG = RegModel.fit(x_train, y_train)
prediction = LREG.predict(x_test)

print("R2 Values:", metrics.r2_score(y_train, LREG.predict(x_train)))

print("\n##### Model Validation and Accuracy Calculator ######")

TestingDataResults = pd.DataFrame(data = x_test, columns = Predictors)
TestingDataResults[TargetVariable] = y_test
TestingDataResults[('Predicted' + TargetVariable)] = np.round(prediction)

print(TestingDataResults.head())

TestingDataResults["APE"] = 100 * ((abs(
TestingDataResults["price"]-TestingDataResults["Predictedprice"]))/TestingDataResults["price"])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])
Accuracy = 100 - MAPE
MedianAccuracy= 100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

Accuracy_Values=cross_val_score(RegModel, x , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Step 21: AdaBoost Algorrithm For ML/AL Model Building"""

from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import make_scorer
from sklearn.model_selection import cross_val_score

DTR=DecisionTreeRegressor(max_depth=3)
RegModel = AdaBoostRegressor(n_estimators=100, base_estimator=DTR ,learning_rate=0.04)

print(RegModel)

AB = RegModel.fit(x_train,y_train)
prediction = AB.predict(x_test)

print('R2 Value:',metrics.r2_score(y_train, AB.predict(x_train)))

feature_importances = pd.Series(AB.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')

print('\n##### Model Validation and Accuracy Calculations ##########')

TestingDataResults=pd.DataFrame(data = x_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

print(TestingDataResults.head())

TestingDataResults['APE'] = 100 * ((abs(TestingDataResults["price"]-TestingDataResults["Predictedprice"]))/TestingDataResults["price"])
MAPE = np.mean(TestingDataResults['APE'])
MedianMAPE = np.median(TestingDataResults['APE'])
Accuracy = 100 - MAPE
MedianAccuracy = 100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy)
print('Median Accuracy on test data:', MedianAccuracy)

def Accuracy_Score(orig,pred):
  MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
  return(100-MAPE)

custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

Accuracy_Values=cross_val_score(RegModel, x , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Pickle the Trained Model"""

import pickle

with open('final_model.pkl', 'wb') as file:
    pickle.dump(LREG, file)

"""# References

*   SETU06. "Handling With Highly Skewed Data Set." Kaggle.com. Accessed: April 29, 2024. [Online]. Available: [link text](https://www.kaggle.com/code/setu06/handling-with-highly-skewed-data-set)
*   Einstein EBEREONWU. "Saving Your Machine Learning Model In Python: pickle.dump()." Medium.com. Accessed: April 29, 2024. [Online]. Available: [link text](https://medium.com/@einsteinmunachiso/saving-your-machine-learning-model-in-python-pickle-dump-b01ae60a791c)
"""
